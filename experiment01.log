2022-05-29 14:07:39,073 - CIFAR10 Classifier - INFO - Experiment (Training Phase): CIFAR10 Classifier
2022-05-29 14:07:39,076 - CIFAR10 Classifier - INFO - Model : 
ResNet152Wrapper(
  (resnet152): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (23): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (24): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (25): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (26): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (27): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (28): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (29): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (30): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (31): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (32): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (33): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (34): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (35): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=10, bias=True)
  )
)
2022-05-29 14:07:39,077 - CIFAR10 Classifier - INFO - Devices : cuda:0
2022-05-29 14:07:39,077 - CIFAR10 Classifier - INFO - Devices ID : [0]
2022-05-29 14:07:39,077 - CIFAR10 Classifier - INFO - See detail of experiment at saved/CIFAR10 Classifier/ResNet152Wrapper/train/config.json
2022-05-29 14:07:40,891 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 0 |  Loss: (0.0000) | Acc: (100.00%) (64/64)
2022-05-29 14:07:44,159 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 10 |  Loss: (0.0000) | Acc: (100.00%) (704/704)
2022-05-29 14:07:47,430 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 20 |  Loss: (0.0001) | Acc: (100.00%) (1344/1344)
2022-05-29 14:07:50,708 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 30 |  Loss: (0.0001) | Acc: (100.00%) (1984/1984)
2022-05-29 14:07:53,994 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 40 |  Loss: (0.0001) | Acc: (100.00%) (2624/2624)
2022-05-29 14:07:57,274 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 50 |  Loss: (0.0001) | Acc: (100.00%) (3264/3264)
2022-05-29 14:08:00,559 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 60 |  Loss: (0.0001) | Acc: (100.00%) (3904/3904)
2022-05-29 14:08:03,847 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 70 |  Loss: (0.0001) | Acc: (100.00%) (4544/4544)
2022-05-29 14:08:07,137 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 80 |  Loss: (0.0001) | Acc: (100.00%) (5184/5184)
2022-05-29 14:08:10,420 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 90 |  Loss: (0.0001) | Acc: (100.00%) (5824/5824)
2022-05-29 14:08:13,703 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 100 |  Loss: (0.0001) | Acc: (100.00%) (6464/6464)
2022-05-29 14:08:16,987 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 110 |  Loss: (0.0001) | Acc: (100.00%) (7104/7104)
2022-05-29 14:08:20,269 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 120 |  Loss: (0.0001) | Acc: (100.00%) (7744/7744)
2022-05-29 14:08:23,554 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 130 |  Loss: (0.0001) | Acc: (100.00%) (8384/8384)
2022-05-29 14:08:26,844 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 140 |  Loss: (0.0001) | Acc: (100.00%) (9024/9024)
2022-05-29 14:08:30,135 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 150 |  Loss: (0.0001) | Acc: (100.00%) (9664/9664)
2022-05-29 14:08:33,427 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 160 |  Loss: (0.0001) | Acc: (100.00%) (10304/10304)
2022-05-29 14:08:36,721 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 170 |  Loss: (0.0001) | Acc: (100.00%) (10944/10944)
2022-05-29 14:08:40,013 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 180 |  Loss: (0.0001) | Acc: (100.00%) (11584/11584)
2022-05-29 14:08:43,307 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 190 |  Loss: (0.0001) | Acc: (100.00%) (12224/12224)
2022-05-29 14:08:46,599 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 200 |  Loss: (0.0001) | Acc: (100.00%) (12864/12864)
2022-05-29 14:08:49,891 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 210 |  Loss: (0.0001) | Acc: (100.00%) (13504/13504)
2022-05-29 14:08:53,183 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 220 |  Loss: (0.0001) | Acc: (100.00%) (14144/14144)
2022-05-29 14:08:56,479 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 230 |  Loss: (0.0001) | Acc: (100.00%) (14784/14784)
2022-05-29 14:08:59,774 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 240 |  Loss: (0.0001) | Acc: (100.00%) (15424/15424)
2022-05-29 14:09:03,065 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 250 |  Loss: (0.0001) | Acc: (100.00%) (16064/16064)
2022-05-29 14:09:06,358 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 260 |  Loss: (0.0001) | Acc: (100.00%) (16704/16704)
2022-05-29 14:09:09,654 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 270 |  Loss: (0.0001) | Acc: (100.00%) (17344/17344)
2022-05-29 14:09:12,947 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 280 |  Loss: (0.0001) | Acc: (100.00%) (17984/17984)
2022-05-29 14:09:16,239 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 290 |  Loss: (0.0001) | Acc: (100.00%) (18624/18624)
2022-05-29 14:09:19,533 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 300 |  Loss: (0.0001) | Acc: (100.00%) (19264/19264)
2022-05-29 14:09:22,829 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 310 |  Loss: (0.0001) | Acc: (100.00%) (19904/19904)
2022-05-29 14:09:26,122 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 320 |  Loss: (0.0001) | Acc: (100.00%) (20544/20544)
2022-05-29 14:09:29,428 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 330 |  Loss: (0.0001) | Acc: (100.00%) (21184/21184)
2022-05-29 14:09:32,732 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 340 |  Loss: (0.0001) | Acc: (100.00%) (21824/21824)
2022-05-29 14:09:36,035 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 350 |  Loss: (0.0001) | Acc: (100.00%) (22464/22464)
2022-05-29 14:09:39,340 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 360 |  Loss: (0.0001) | Acc: (100.00%) (23104/23104)
2022-05-29 14:09:42,642 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 370 |  Loss: (0.0001) | Acc: (100.00%) (23744/23744)
2022-05-29 14:09:45,946 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 380 |  Loss: (0.0001) | Acc: (100.00%) (24384/24384)
2022-05-29 14:09:49,251 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 390 |  Loss: (0.0001) | Acc: (100.00%) (25024/25024)
2022-05-29 14:09:52,556 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 400 |  Loss: (0.0001) | Acc: (100.00%) (25664/25664)
2022-05-29 14:09:55,860 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 410 |  Loss: (0.0001) | Acc: (100.00%) (26304/26304)
2022-05-29 14:09:59,161 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 420 |  Loss: (0.0001) | Acc: (100.00%) (26944/26944)
2022-05-29 14:10:02,463 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 430 |  Loss: (0.0001) | Acc: (100.00%) (27584/27584)
2022-05-29 14:10:05,769 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 440 |  Loss: (0.0001) | Acc: (100.00%) (28224/28224)
2022-05-29 14:10:09,071 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 450 |  Loss: (0.0001) | Acc: (100.00%) (28864/28864)
2022-05-29 14:10:12,373 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 460 |  Loss: (0.0001) | Acc: (100.00%) (29504/29504)
2022-05-29 14:10:15,676 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 470 |  Loss: (0.0001) | Acc: (100.00%) (30144/30144)
2022-05-29 14:10:18,978 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 480 |  Loss: (0.0001) | Acc: (100.00%) (30784/30784)
2022-05-29 14:10:22,280 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 490 |  Loss: (0.0001) | Acc: (100.00%) (31424/31424)
2022-05-29 14:10:25,581 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 500 |  Loss: (0.0001) | Acc: (100.00%) (32064/32064)
2022-05-29 14:10:28,885 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 510 |  Loss: (0.0001) | Acc: (100.00%) (32704/32704)
2022-05-29 14:10:32,187 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 520 |  Loss: (0.0001) | Acc: (100.00%) (33344/33344)
2022-05-29 14:10:35,493 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 530 |  Loss: (0.0002) | Acc: (100.00%) (33984/33984)
2022-05-29 14:10:38,798 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 540 |  Loss: (0.0002) | Acc: (100.00%) (34624/34624)
2022-05-29 14:10:42,099 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 550 |  Loss: (0.0002) | Acc: (100.00%) (35264/35264)
2022-05-29 14:10:45,402 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 560 |  Loss: (0.0002) | Acc: (100.00%) (35904/35904)
2022-05-29 14:10:48,705 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 570 |  Loss: (0.0002) | Acc: (100.00%) (36544/36544)
2022-05-29 14:10:52,007 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 580 |  Loss: (0.0002) | Acc: (100.00%) (37184/37184)
2022-05-29 14:10:55,309 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 590 |  Loss: (0.0002) | Acc: (100.00%) (37824/37824)
2022-05-29 14:10:58,609 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 600 |  Loss: (0.0002) | Acc: (100.00%) (38464/38464)
2022-05-29 14:11:01,910 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 610 |  Loss: (0.0002) | Acc: (100.00%) (39104/39104)
2022-05-29 14:11:05,211 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 620 |  Loss: (0.0002) | Acc: (100.00%) (39744/39744)
2022-05-29 14:11:08,514 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 630 |  Loss: (0.0002) | Acc: (100.00%) (40384/40384)
2022-05-29 14:11:11,817 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 640 |  Loss: (0.0002) | Acc: (100.00%) (41024/41024)
2022-05-29 14:11:15,118 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 650 |  Loss: (0.0002) | Acc: (100.00%) (41664/41664)
2022-05-29 14:11:18,421 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 660 |  Loss: (0.0002) | Acc: (100.00%) (42304/42304)
2022-05-29 14:11:21,723 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 670 |  Loss: (0.0002) | Acc: (100.00%) (42944/42944)
2022-05-29 14:11:25,026 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 680 |  Loss: (0.0002) | Acc: (100.00%) (43584/43584)
2022-05-29 14:11:28,329 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 690 |  Loss: (0.0002) | Acc: (100.00%) (44224/44224)
2022-05-29 14:11:31,629 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 700 |  Loss: (0.0002) | Acc: (100.00%) (44864/44864)
2022-05-29 14:11:34,930 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 710 |  Loss: (0.0002) | Acc: (100.00%) (45504/45504)
2022-05-29 14:11:38,233 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 720 |  Loss: (0.0002) | Acc: (100.00%) (46144/46144)
2022-05-29 14:11:41,534 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 730 |  Loss: (0.0002) | Acc: (100.00%) (46784/46784)
2022-05-29 14:11:44,838 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 740 |  Loss: (0.0002) | Acc: (100.00%) (47424/47424)
2022-05-29 14:11:48,142 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 750 |  Loss: (0.0002) | Acc: (100.00%) (48064/48064)
2022-05-29 14:11:51,444 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 760 |  Loss: (0.0002) | Acc: (100.00%) (48704/48704)
2022-05-29 14:11:54,741 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 770 |  Loss: (0.0002) | Acc: (100.00%) (49344/49344)
2022-05-29 14:11:58,041 - CIFAR10 Classifier - INFO - Epoch: 0 | Batch_idx: 780 |  Loss: (0.0002) | Acc: (100.00%) (49984/49984)
2022-05-29 14:12:17,229 - CIFAR10 Classifier - INFO - # TEST : Loss: (0.0827) | Acc: (97.64%) (9764/10000)
2022-05-29 14:12:17,230 - CIFAR10 Classifier - INFO - Epoch time : 0:04:37
2022-05-29 14:12:19,094 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 0 |  Loss: (0.0010) | Acc: (100.00%) (64/64)
2022-05-29 14:12:22,389 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 10 |  Loss: (0.0008) | Acc: (100.00%) (704/704)
2022-05-29 14:12:25,684 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 20 |  Loss: (0.0008) | Acc: (100.00%) (1344/1344)
2022-05-29 14:12:28,979 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 30 |  Loss: (0.0007) | Acc: (100.00%) (1984/1984)
2022-05-29 14:12:32,276 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 40 |  Loss: (0.0007) | Acc: (100.00%) (2624/2624)
2022-05-29 14:12:35,573 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 50 |  Loss: (0.0007) | Acc: (100.00%) (3264/3264)
2022-05-29 14:12:38,872 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 60 |  Loss: (0.0006) | Acc: (100.00%) (3904/3904)
2022-05-29 14:12:42,171 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 70 |  Loss: (0.0006) | Acc: (100.00%) (4544/4544)
2022-05-29 14:12:45,468 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 80 |  Loss: (0.0006) | Acc: (100.00%) (5184/5184)
2022-05-29 14:12:48,767 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 90 |  Loss: (0.0006) | Acc: (100.00%) (5824/5824)
2022-05-29 14:12:52,064 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 100 |  Loss: (0.0007) | Acc: (100.00%) (6464/6464)
2022-05-29 14:12:55,365 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 110 |  Loss: (0.0007) | Acc: (100.00%) (7104/7104)
2022-05-29 14:12:58,663 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 120 |  Loss: (0.0007) | Acc: (100.00%) (7744/7744)
2022-05-29 14:13:01,963 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 130 |  Loss: (0.0007) | Acc: (100.00%) (8384/8384)
2022-05-29 14:13:05,261 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 140 |  Loss: (0.0007) | Acc: (100.00%) (9024/9024)
2022-05-29 14:13:08,561 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 150 |  Loss: (0.0007) | Acc: (100.00%) (9664/9664)
2022-05-29 14:13:11,859 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 160 |  Loss: (0.0008) | Acc: (99.99%) (10303/10304)
2022-05-29 14:13:15,157 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 170 |  Loss: (0.0008) | Acc: (99.99%) (10943/10944)
2022-05-29 14:13:18,455 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 180 |  Loss: (0.0008) | Acc: (99.99%) (11583/11584)
2022-05-29 14:13:21,751 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 190 |  Loss: (0.0008) | Acc: (99.99%) (12223/12224)
2022-05-29 14:13:25,046 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 200 |  Loss: (0.0008) | Acc: (99.99%) (12863/12864)
2022-05-29 14:13:28,345 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 210 |  Loss: (0.0008) | Acc: (99.99%) (13503/13504)
2022-05-29 14:13:31,643 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 220 |  Loss: (0.0008) | Acc: (99.99%) (14143/14144)
2022-05-29 14:13:34,946 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 230 |  Loss: (0.0008) | Acc: (99.99%) (14783/14784)
2022-05-29 14:13:38,254 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 240 |  Loss: (0.0008) | Acc: (99.99%) (15423/15424)
2022-05-29 14:13:41,562 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 250 |  Loss: (0.0008) | Acc: (99.99%) (16063/16064)
2022-05-29 14:13:44,868 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 260 |  Loss: (0.0009) | Acc: (99.99%) (16703/16704)
2022-05-29 14:13:48,175 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 270 |  Loss: (0.0009) | Acc: (99.99%) (17343/17344)
2022-05-29 14:13:51,479 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 280 |  Loss: (0.0009) | Acc: (99.99%) (17983/17984)
2022-05-29 14:13:54,783 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 290 |  Loss: (0.0009) | Acc: (99.99%) (18623/18624)
2022-05-29 14:13:58,088 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 300 |  Loss: (0.0009) | Acc: (99.99%) (19263/19264)
2022-05-29 14:14:01,394 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 310 |  Loss: (0.0009) | Acc: (99.99%) (19903/19904)
2022-05-29 14:14:04,702 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 320 |  Loss: (0.0009) | Acc: (100.00%) (20543/20544)
2022-05-29 14:14:08,007 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 330 |  Loss: (0.0009) | Acc: (100.00%) (21183/21184)
2022-05-29 14:14:11,316 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 340 |  Loss: (0.0009) | Acc: (100.00%) (21823/21824)
2022-05-29 14:14:14,618 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 350 |  Loss: (0.0009) | Acc: (100.00%) (22463/22464)
2022-05-29 14:14:17,920 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 360 |  Loss: (0.0009) | Acc: (100.00%) (23103/23104)
2022-05-29 14:14:21,223 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 370 |  Loss: (0.0009) | Acc: (100.00%) (23743/23744)
2022-05-29 14:14:24,526 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 380 |  Loss: (0.0010) | Acc: (100.00%) (24383/24384)
2022-05-29 14:14:27,831 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 390 |  Loss: (0.0010) | Acc: (100.00%) (25023/25024)
2022-05-29 14:14:31,134 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 400 |  Loss: (0.0010) | Acc: (100.00%) (25663/25664)
2022-05-29 14:14:34,438 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 410 |  Loss: (0.0010) | Acc: (100.00%) (26303/26304)
2022-05-29 14:14:37,738 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 420 |  Loss: (0.0010) | Acc: (100.00%) (26943/26944)
2022-05-29 14:14:41,042 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 430 |  Loss: (0.0010) | Acc: (100.00%) (27583/27584)
2022-05-29 14:14:44,343 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 440 |  Loss: (0.0010) | Acc: (100.00%) (28223/28224)
2022-05-29 14:14:47,643 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 450 |  Loss: (0.0010) | Acc: (100.00%) (28863/28864)
2022-05-29 14:14:50,947 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 460 |  Loss: (0.0010) | Acc: (100.00%) (29503/29504)
2022-05-29 14:14:54,253 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 470 |  Loss: (0.0010) | Acc: (100.00%) (30143/30144)
2022-05-29 14:14:57,561 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 480 |  Loss: (0.0010) | Acc: (100.00%) (30783/30784)
2022-05-29 14:15:00,863 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 490 |  Loss: (0.0010) | Acc: (100.00%) (31423/31424)
2022-05-29 14:15:04,165 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 500 |  Loss: (0.0010) | Acc: (100.00%) (32063/32064)
2022-05-29 14:15:07,467 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 510 |  Loss: (0.0010) | Acc: (100.00%) (32703/32704)
2022-05-29 14:15:10,771 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 520 |  Loss: (0.0010) | Acc: (100.00%) (33343/33344)
2022-05-29 14:15:14,074 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 530 |  Loss: (0.0011) | Acc: (100.00%) (33983/33984)
2022-05-29 14:15:17,376 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 540 |  Loss: (0.0011) | Acc: (100.00%) (34623/34624)
2022-05-29 14:15:20,677 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 550 |  Loss: (0.0011) | Acc: (100.00%) (35263/35264)
2022-05-29 14:15:23,980 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 560 |  Loss: (0.0011) | Acc: (100.00%) (35903/35904)
2022-05-29 14:15:27,283 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 570 |  Loss: (0.0011) | Acc: (100.00%) (36543/36544)
2022-05-29 14:15:30,585 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 580 |  Loss: (0.0011) | Acc: (100.00%) (37183/37184)
2022-05-29 14:15:33,888 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 590 |  Loss: (0.0011) | Acc: (100.00%) (37823/37824)
2022-05-29 14:15:37,187 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 600 |  Loss: (0.0011) | Acc: (100.00%) (38463/38464)
2022-05-29 14:15:40,489 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 610 |  Loss: (0.0011) | Acc: (100.00%) (39103/39104)
2022-05-29 14:15:43,792 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 620 |  Loss: (0.0011) | Acc: (100.00%) (39743/39744)
2022-05-29 14:15:47,089 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 630 |  Loss: (0.0011) | Acc: (100.00%) (40383/40384)
2022-05-29 14:15:50,387 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 640 |  Loss: (0.0012) | Acc: (100.00%) (41023/41024)
2022-05-29 14:15:53,684 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 650 |  Loss: (0.0012) | Acc: (100.00%) (41663/41664)
2022-05-29 14:15:56,984 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 660 |  Loss: (0.0012) | Acc: (100.00%) (42303/42304)
2022-05-29 14:16:00,282 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 670 |  Loss: (0.0012) | Acc: (100.00%) (42943/42944)
2022-05-29 14:16:03,589 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 680 |  Loss: (0.0012) | Acc: (100.00%) (43583/43584)
2022-05-29 14:16:06,897 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 690 |  Loss: (0.0012) | Acc: (100.00%) (44223/44224)
2022-05-29 14:16:10,203 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 700 |  Loss: (0.0012) | Acc: (100.00%) (44863/44864)
2022-05-29 14:16:13,513 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 710 |  Loss: (0.0012) | Acc: (100.00%) (45503/45504)
2022-05-29 14:16:16,820 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 720 |  Loss: (0.0012) | Acc: (100.00%) (46143/46144)
2022-05-29 14:16:20,126 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 730 |  Loss: (0.0013) | Acc: (100.00%) (46783/46784)
2022-05-29 14:16:23,432 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 740 |  Loss: (0.0013) | Acc: (100.00%) (47423/47424)
2022-05-29 14:16:26,737 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 750 |  Loss: (0.0013) | Acc: (100.00%) (48063/48064)
2022-05-29 14:16:30,045 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 760 |  Loss: (0.0013) | Acc: (100.00%) (48703/48704)
2022-05-29 14:16:33,340 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 770 |  Loss: (0.0013) | Acc: (100.00%) (49343/49344)
2022-05-29 14:16:36,636 - CIFAR10 Classifier - INFO - Epoch: 1 | Batch_idx: 780 |  Loss: (0.0013) | Acc: (100.00%) (49983/49984)
2022-05-29 14:16:55,868 - CIFAR10 Classifier - INFO - # TEST : Loss: (0.0813) | Acc: (97.46%) (9746/10000)
2022-05-29 14:16:55,869 - CIFAR10 Classifier - INFO - Epoch time : 0:04:37
2022-05-29 14:16:57,649 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 0 |  Loss: (0.0016) | Acc: (100.00%) (64/64)
2022-05-29 14:17:00,946 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 10 |  Loss: (0.0014) | Acc: (100.00%) (704/704)
2022-05-29 14:17:04,239 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 20 |  Loss: (0.0020) | Acc: (100.00%) (1344/1344)
2022-05-29 14:17:07,536 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 30 |  Loss: (0.0021) | Acc: (100.00%) (1984/1984)
2022-05-29 14:17:10,832 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 40 |  Loss: (0.0020) | Acc: (100.00%) (2624/2624)
2022-05-29 14:17:14,128 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 50 |  Loss: (0.0020) | Acc: (100.00%) (3264/3264)
2022-05-29 14:17:17,424 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 60 |  Loss: (0.0019) | Acc: (100.00%) (3904/3904)
2022-05-29 14:17:20,725 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 70 |  Loss: (0.0019) | Acc: (100.00%) (4544/4544)
2022-05-29 14:17:24,020 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 80 |  Loss: (0.0020) | Acc: (100.00%) (5184/5184)
2022-05-29 14:17:27,317 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 90 |  Loss: (0.0019) | Acc: (100.00%) (5824/5824)
2022-05-29 14:17:30,612 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 100 |  Loss: (0.0019) | Acc: (100.00%) (6464/6464)
2022-05-29 14:17:33,912 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 110 |  Loss: (0.0020) | Acc: (100.00%) (7104/7104)
2022-05-29 14:17:37,210 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 120 |  Loss: (0.0021) | Acc: (100.00%) (7744/7744)
2022-05-29 14:17:40,507 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 130 |  Loss: (0.0021) | Acc: (100.00%) (8384/8384)
2022-05-29 14:17:43,803 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 140 |  Loss: (0.0021) | Acc: (100.00%) (9024/9024)
2022-05-29 14:17:47,100 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 150 |  Loss: (0.0021) | Acc: (100.00%) (9664/9664)
2022-05-29 14:17:50,398 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 160 |  Loss: (0.0021) | Acc: (100.00%) (10304/10304)
2022-05-29 14:17:53,696 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 170 |  Loss: (0.0021) | Acc: (100.00%) (10944/10944)
2022-05-29 14:17:56,993 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 180 |  Loss: (0.0021) | Acc: (100.00%) (11584/11584)
2022-05-29 14:18:00,293 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 190 |  Loss: (0.0021) | Acc: (100.00%) (12224/12224)
2022-05-29 14:18:03,591 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 200 |  Loss: (0.0022) | Acc: (99.99%) (12863/12864)
2022-05-29 14:18:06,896 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 210 |  Loss: (0.0022) | Acc: (99.99%) (13503/13504)
2022-05-29 14:18:10,202 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 220 |  Loss: (0.0022) | Acc: (99.99%) (14143/14144)
2022-05-29 14:18:13,506 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 230 |  Loss: (0.0022) | Acc: (99.99%) (14783/14784)
2022-05-29 14:18:16,812 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 240 |  Loss: (0.0022) | Acc: (99.99%) (15423/15424)
2022-05-29 14:18:20,119 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 250 |  Loss: (0.0022) | Acc: (99.99%) (16063/16064)
2022-05-29 14:18:23,424 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 260 |  Loss: (0.0023) | Acc: (99.99%) (16703/16704)
2022-05-29 14:18:26,730 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 270 |  Loss: (0.0023) | Acc: (99.99%) (17343/17344)
2022-05-29 14:18:30,037 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 280 |  Loss: (0.0024) | Acc: (99.99%) (17983/17984)
2022-05-29 14:18:33,344 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 290 |  Loss: (0.0024) | Acc: (99.99%) (18623/18624)
2022-05-29 14:18:36,654 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 300 |  Loss: (0.0024) | Acc: (99.99%) (19263/19264)
2022-05-29 14:18:39,963 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 310 |  Loss: (0.0025) | Acc: (99.99%) (19903/19904)
2022-05-29 14:18:43,274 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 320 |  Loss: (0.0025) | Acc: (100.00%) (20543/20544)
2022-05-29 14:18:46,583 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 330 |  Loss: (0.0025) | Acc: (100.00%) (21183/21184)
2022-05-29 14:18:49,891 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 340 |  Loss: (0.0025) | Acc: (100.00%) (21823/21824)
2022-05-29 14:18:53,197 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 350 |  Loss: (0.0026) | Acc: (100.00%) (22463/22464)
2022-05-29 14:18:56,497 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 360 |  Loss: (0.0026) | Acc: (100.00%) (23103/23104)
2022-05-29 14:18:59,796 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 370 |  Loss: (0.0027) | Acc: (99.99%) (23742/23744)
2022-05-29 14:19:03,096 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 380 |  Loss: (0.0027) | Acc: (99.99%) (24382/24384)
2022-05-29 14:19:06,394 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 390 |  Loss: (0.0028) | Acc: (99.99%) (25022/25024)
2022-05-29 14:19:09,691 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 400 |  Loss: (0.0028) | Acc: (99.99%) (25662/25664)
2022-05-29 14:19:12,989 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 410 |  Loss: (0.0028) | Acc: (99.99%) (26302/26304)
2022-05-29 14:19:16,287 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 420 |  Loss: (0.0028) | Acc: (99.99%) (26942/26944)
2022-05-29 14:19:19,587 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 430 |  Loss: (0.0029) | Acc: (99.99%) (27582/27584)
2022-05-29 14:19:22,886 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 440 |  Loss: (0.0029) | Acc: (99.99%) (28222/28224)
2022-05-29 14:19:26,184 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 450 |  Loss: (0.0029) | Acc: (99.99%) (28862/28864)
2022-05-29 14:19:29,483 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 460 |  Loss: (0.0029) | Acc: (99.99%) (29502/29504)
2022-05-29 14:19:32,783 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 470 |  Loss: (0.0030) | Acc: (99.99%) (30142/30144)
2022-05-29 14:19:36,081 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 480 |  Loss: (0.0030) | Acc: (99.99%) (30781/30784)
2022-05-29 14:19:39,380 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 490 |  Loss: (0.0030) | Acc: (99.99%) (31421/31424)
2022-05-29 14:19:42,680 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 500 |  Loss: (0.0030) | Acc: (99.99%) (32061/32064)
2022-05-29 14:19:45,981 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 510 |  Loss: (0.0031) | Acc: (99.99%) (32701/32704)
2022-05-29 14:19:49,282 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 520 |  Loss: (0.0031) | Acc: (99.99%) (33341/33344)
2022-05-29 14:19:52,581 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 530 |  Loss: (0.0031) | Acc: (99.99%) (33981/33984)
2022-05-29 14:19:55,880 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 540 |  Loss: (0.0031) | Acc: (99.99%) (34621/34624)
2022-05-29 14:19:59,182 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 550 |  Loss: (0.0032) | Acc: (99.99%) (35260/35264)
2022-05-29 14:20:02,485 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 560 |  Loss: (0.0033) | Acc: (99.99%) (35899/35904)
2022-05-29 14:20:05,784 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 570 |  Loss: (0.0033) | Acc: (99.98%) (36538/36544)
2022-05-29 14:20:09,083 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 580 |  Loss: (0.0033) | Acc: (99.98%) (37178/37184)
2022-05-29 14:20:12,382 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 590 |  Loss: (0.0033) | Acc: (99.98%) (37818/37824)
2022-05-29 14:20:15,682 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 600 |  Loss: (0.0034) | Acc: (99.98%) (38458/38464)
2022-05-29 14:20:18,982 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 610 |  Loss: (0.0034) | Acc: (99.98%) (39097/39104)
2022-05-29 14:20:22,281 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 620 |  Loss: (0.0035) | Acc: (99.98%) (39737/39744)
2022-05-29 14:20:25,580 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 630 |  Loss: (0.0035) | Acc: (99.98%) (40377/40384)
2022-05-29 14:20:28,880 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 640 |  Loss: (0.0035) | Acc: (99.98%) (41017/41024)
2022-05-29 14:20:32,181 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 650 |  Loss: (0.0036) | Acc: (99.98%) (41657/41664)
2022-05-29 14:20:35,481 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 660 |  Loss: (0.0036) | Acc: (99.98%) (42297/42304)
2022-05-29 14:20:38,780 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 670 |  Loss: (0.0036) | Acc: (99.98%) (42937/42944)
2022-05-29 14:20:42,080 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 680 |  Loss: (0.0036) | Acc: (99.98%) (43575/43584)
2022-05-29 14:20:45,380 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 690 |  Loss: (0.0037) | Acc: (99.98%) (44215/44224)
2022-05-29 14:20:48,679 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 700 |  Loss: (0.0037) | Acc: (99.98%) (44854/44864)
2022-05-29 14:20:51,978 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 710 |  Loss: (0.0038) | Acc: (99.98%) (45493/45504)
2022-05-29 14:20:55,277 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 720 |  Loss: (0.0039) | Acc: (99.97%) (46132/46144)
2022-05-29 14:20:58,578 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 730 |  Loss: (0.0039) | Acc: (99.97%) (46772/46784)
2022-05-29 14:21:01,877 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 740 |  Loss: (0.0039) | Acc: (99.97%) (47412/47424)
2022-05-29 14:21:05,176 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 750 |  Loss: (0.0039) | Acc: (99.98%) (48052/48064)
2022-05-29 14:21:08,479 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 760 |  Loss: (0.0040) | Acc: (99.98%) (48692/48704)
2022-05-29 14:21:11,772 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 770 |  Loss: (0.0040) | Acc: (99.98%) (49332/49344)
2022-05-29 14:21:15,064 - CIFAR10 Classifier - INFO - Epoch: 2 | Batch_idx: 780 |  Loss: (0.0040) | Acc: (99.98%) (49972/49984)
2022-05-29 14:21:34,278 - CIFAR10 Classifier - INFO - # TEST : Loss: (0.0918) | Acc: (97.16%) (9716/10000)
2022-05-29 14:21:34,279 - CIFAR10 Classifier - INFO - Epoch time : 0:04:37
2022-05-29 14:21:36,119 - CIFAR10 Classifier - INFO - Epoch: 3 | Batch_idx: 0 |  Loss: (0.0068) | Acc: (100.00%) (64/64)
2022-05-29 14:21:39,411 - CIFAR10 Classifier - INFO - Epoch: 3 | Batch_idx: 10 |  Loss: (0.0049) | Acc: (100.00%) (704/704)
